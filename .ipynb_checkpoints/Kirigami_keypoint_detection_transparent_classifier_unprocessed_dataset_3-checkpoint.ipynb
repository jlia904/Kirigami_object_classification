{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages \n",
    "\n",
    "import torch \n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import random\n",
    "import albumentations as A\n",
    "import glob\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.Sequential(\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3200, 300),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(300, 32),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_model()\n",
    "model.load_state_dict(torch.load('./Model_VGG_4_5_6_2000_16300_epoch'))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_keypoints(img):\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img_tensor = transforms.ToTensor()(img)\n",
    "    img_mean = img_tensor.mean(dim = (1,2))\n",
    "    img_std = img_tensor.std(dim = (1,2))\n",
    "        \n",
    "    img_normalised = transforms.Normalize(img_mean, img_std)(img_tensor)\n",
    "    img_normalised = img_normalised.to(device)\n",
    "    \n",
    "    key_points = model(img_normalised[None]).flatten().detach().cpu().numpy()\n",
    "    \n",
    "    return key_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints(img, keypoints):                                                             \n",
    "\n",
    "    plt.imshow(img)\n",
    "\n",
    "    x_points = keypoints[0::2]\n",
    "    y_points = keypoints[1::2]\n",
    "    plt.scatter(x_points*img.shape[1], y_points*img.shape[0], s = 4, c=(1,0,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Grasp_dataset_eyedropper', 'Grasp_dataset_roundbottlelarge', 'Grasp_dataset_roundbottlemedium', 'Grasp_dataset_roundbottlesmall', 'Grasp_dataset_squarebottle', 'Grasp_dataset_testtube']\n",
    "\n",
    "header = ['weight_reading_1', 'weight_reading_2', 'pressure_reading_1', 'pressure_reading_2', 'force_reading_1', 'force_reading_2', 'p1_x', 'p1_y', 'p2_x', 'p2_y', 'p3_x', 'p3_y', 'p4_x', 'p4_y', 'p5_x', 'p5_y', 'p6_x', 'p6_y', 'p7_x', 'p7_y', 'p8_x', 'p8_y', 'p9_x', 'p9_y', 'p10_x', 'p10_y', 'p11_x', 'p11_y', 'p12_x', 'p12_y', 'p13_x', 'p13_y', 'p14_x', 'p14_y', 'p15_x', 'p15_y', 'p16_x', 'p16_y', 'label']\n",
    "\n",
    "current_dir = os. getcwd()\n",
    "\n",
    "weight_reading_1_arr = []\n",
    "weight_reading_2_arr = []\n",
    "pressure_reading_1_arr = []\n",
    "pressure_reading_2_arr = []\n",
    "force_reading_1_arr = []\n",
    "force_reading_2_arr = []\n",
    "\n",
    "p1_x_arr = []\n",
    "p1_y_arr = []\n",
    "p2_x_arr = []\n",
    "p2_y_arr = []\n",
    "p3_x_arr = []\n",
    "p3_y_arr = []\n",
    "p4_x_arr = []\n",
    "p4_y_arr = []\n",
    "p5_x_arr = []\n",
    "p5_y_arr = []\n",
    "p6_x_arr = []\n",
    "p6_y_arr = []\n",
    "p7_x_arr = []\n",
    "p7_y_arr = []\n",
    "p8_x_arr = []\n",
    "p8_y_arr = []\n",
    "p9_x_arr = []\n",
    "p9_y_arr = []\n",
    "p10_x_arr = []\n",
    "p10_y_arr = []\n",
    "p11_x_arr = []\n",
    "p11_y_arr = []\n",
    "p12_x_arr = []\n",
    "p12_y_arr = []\n",
    "p13_x_arr = []\n",
    "p13_y_arr = []\n",
    "p14_x_arr = []\n",
    "p14_y_arr = []\n",
    "p15_x_arr = []\n",
    "p15_y_arr = []\n",
    "p16_x_arr = []\n",
    "p16_y_arr = []\n",
    "label_arr = []\n",
    "\n",
    "\n",
    "for f in dataset_names:\n",
    "    \n",
    "    image_dir = os.path.join(current_dir + '/Grasp_dataset_3', f)\n",
    "    data_dir = os.path.join(image_dir, f + '.csv')\n",
    "    df = pd.read_csv(data_dir)\n",
    "    weight_reading_1_arr = np.concatenate((weight_reading_1_arr, df['weight_reading_1'].values))\n",
    "    weight_reading_2_arr = np.concatenate((weight_reading_2_arr, df['weight_reading_2'].values))\n",
    "    pressure_reading_1_arr = np.concatenate((pressure_reading_1_arr, df['pressure_reading_1'].values))\n",
    "    pressure_reading_2_arr = np.concatenate((pressure_reading_2_arr, df['pressure_reading_2'].values))\n",
    "    force_reading_1_arr = np.concatenate((force_reading_1_arr, df['force_reading_1'].values))\n",
    "    force_reading_2_arr = np.concatenate((force_reading_2_arr, df['force_reading_2'].values))\n",
    "    label_arr = np.concatenate((label_arr, df['label'].values))\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    while j < df.shape[0]:\n",
    "        image = cv2.imread(os.path.join(image_dir, df.iloc[j]['image_name']))\n",
    "        keypoints = predict_keypoints(image)\n",
    "        \n",
    "#         plot_keypoints(image, keypoints)\n",
    "        \n",
    "#         if j == 20:\n",
    "#             plot_keypoints(image, keypoints)\n",
    "        \n",
    "\n",
    "#         plt.imshow(image)\n",
    "\n",
    "#         plt.scatter(kirigami_x_points*image.shape[1], kirigami_y_points*image.shape[0], s = 4, c=(1,0,0))\n",
    "#         plt.show()\n",
    "        \n",
    "        p1_x_arr.append(keypoints[0])\n",
    "        p1_y_arr.append(keypoints[1])\n",
    "        p2_x_arr.append(keypoints[2])\n",
    "        p2_y_arr.append(keypoints[3])\n",
    "        p3_x_arr.append(keypoints[4])\n",
    "        p3_y_arr.append(keypoints[5])\n",
    "        p4_x_arr.append(keypoints[6])\n",
    "        p4_y_arr.append(keypoints[7])\n",
    "        p5_x_arr.append(keypoints[8])\n",
    "        p5_y_arr.append(keypoints[9])\n",
    "        p6_x_arr.append(keypoints[10])\n",
    "        p6_y_arr.append(keypoints[11])\n",
    "        p7_x_arr.append(keypoints[12])\n",
    "        p7_y_arr.append(keypoints[13])\n",
    "        p8_x_arr.append(keypoints[14])\n",
    "        p8_y_arr.append(keypoints[15])\n",
    "        p9_x_arr.append(keypoints[16])\n",
    "        p9_y_arr.append(keypoints[17])\n",
    "        p10_x_arr.append(keypoints[18])\n",
    "        p10_y_arr.append(keypoints[19])\n",
    "        p11_x_arr.append(keypoints[20])\n",
    "        p11_y_arr.append(keypoints[21])\n",
    "        p12_x_arr.append(keypoints[22])\n",
    "        p12_y_arr.append(keypoints[23])\n",
    "        p13_x_arr.append(keypoints[24])\n",
    "        p13_y_arr.append(keypoints[25])\n",
    "        p14_x_arr.append(keypoints[26])\n",
    "        p14_y_arr.append(keypoints[27])\n",
    "        p15_x_arr.append(keypoints[28])\n",
    "        p15_y_arr.append(keypoints[29])\n",
    "        p16_x_arr.append(keypoints[30])\n",
    "        p16_y_arr.append(keypoints[31])\n",
    "        \n",
    "        j = j + 1\n",
    "        \n",
    "#         if j == 2:\n",
    "#             break\n",
    "#     break\n",
    "\n",
    "\n",
    "weight_diff_1_arr = np.array(weight_reading_1_arr) - np.array(force_reading_1_arr)\n",
    "weight_diff_2_arr = np.array(weight_reading_2_arr) - np.array(force_reading_2_arr)\n",
    "\n",
    "grasp_dataset = pd.concat([pd.Series(weight_diff_1_arr), pd.Series(weight_diff_2_arr), pd.Series(pressure_reading_1_arr), pd.Series(pressure_reading_2_arr), pd.Series(force_reading_1_arr), pd.Series(force_reading_2_arr), pd.Series(p1_x_arr), pd.Series(p1_y_arr), pd.Series(p2_x_arr), pd.Series(p2_y_arr), pd.Series(p3_x_arr), pd.Series(p3_y_arr), pd.Series(p4_x_arr), pd.Series(p4_y_arr), pd.Series(p5_x_arr), pd.Series(p5_y_arr), pd.Series(p6_x_arr), pd.Series(p6_y_arr), pd.Series(p7_x_arr), pd.Series(p7_y_arr), pd.Series(p8_x_arr), pd.Series(p8_y_arr), pd.Series(p9_x_arr), pd.Series(p9_y_arr), pd.Series(p10_x_arr), pd.Series(p10_y_arr), pd.Series(p11_x_arr), pd.Series(p11_y_arr), pd.Series(p12_x_arr), pd.Series(p12_y_arr), pd.Series(p13_x_arr), pd.Series(p13_y_arr), pd.Series(p14_x_arr), pd.Series(p14_y_arr), pd.Series(p15_x_arr), pd.Series(p15_y_arr), pd.Series(p16_x_arr), pd.Series(p16_y_arr), pd.Series(label_arr)], axis=1, keys=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "print(grasp_dataset.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b621963",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afcd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_data = grasp_dataset.drop(columns=['label'], axis=1)\n",
    "# grasp_data = grasp_dataset.drop(columns=['label', 'p1_x', 'p1_y', 'p2_x', 'p2_y', 'p3_x', 'p3_y', 'p4_x', 'p4_y', 'p5_x', 'p5_y', 'p6_x', 'p6_y', 'p7_x', 'p7_y', 'p8_x', 'p8_y', 'p9_x', 'p9_y', 'p10_x', 'p10_y', 'p11_x', 'p11_y', 'p12_x', 'p12_y', 'p13_x', 'p13_y', 'p14_x', 'p14_y', 'p15_x', 'p15_y', 'p16_x', 'p16_y'], axis=1)\n",
    "grasp_data['weight_reading_1'] = grasp_data['weight_reading_1'] / 1000\n",
    "grasp_data['weight_reading_2'] = grasp_data['weight_reading_2'] / 1000\n",
    "grasp_data['pressure_reading_1'] = grasp_data['pressure_reading_1'] / 100\n",
    "grasp_data['pressure_reading_2'] = grasp_data['pressure_reading_2'] / 100\n",
    "grasp_data['force_reading_1'] = grasp_data['force_reading_1'] / 1000\n",
    "grasp_data['force_reading_2'] = grasp_data['force_reading_2'] / 1000\n",
    "grasp_label = grasp_dataset['label']\n",
    "\n",
    "print(grasp_data.head(2))\n",
    "print(grasp_label.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(grasp_data, grasp_label, test_size=0.2, random_state=np.random.randint(100))\n",
    "\n",
    "# print(X_train.head(3))\n",
    "\n",
    "# clf = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = knn.predict(X_test)\n",
    "\n",
    "# accuracy = knn.score(X_test, y_test)\n",
    "# print('Accuracy:', accuracy)\n",
    "# clf = RandomForestClassifier(n_estimators=1000, max_depth=50, random_state=np.random.randint(100))\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=200, random_state=np.random.randint(100), n_jobs = -1)\n",
    "# clf = HistGradientBoostingClassifier(max_iter=200, random_state=np.random.randint(100))\n",
    "# clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=1, random_state=0)\n",
    "# clf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='hard')\n",
    "\n",
    "num_folds = 10\n",
    "cv_method = KFold(n_splits=num_folds, shuffle=True, random_state=np.random.randint(100))\n",
    "cv_results = cross_val_score(clf, grasp_data, grasp_label, cv=cv_method, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation results:', cv_results)\n",
    "print('Average accuracy:', cv_results.mean())\n",
    "\n",
    "# clf.fit(data_train, label_train)\n",
    "# label_pred = clf.predict(data_test)\n",
    "# cm = confusion_matrix(label_test, label_pred)\n",
    "\n",
    "# short_dataset_names = []'eyedropper', 'roundbottlelarge', 'roundbottlemedium', 'roundbottlesmall', 'squarebottle', 'testtube']\n",
    "\n",
    "# cm_df = pd.DataFrame(cm, index=short_dataset_names, columns=short_dataset_names)\n",
    "\n",
    "# print(cm_df)\n",
    "# print(cm)\n",
    "\n",
    "# accuracy = clf.score(data_test, label_test)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9281437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982fb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1bc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d25dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427cdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672ddbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84543cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Keypoint_detection_notebooks",
   "language": "python",
   "name": ".keypoint_detection_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
