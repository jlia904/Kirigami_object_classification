{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbb8b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/Desktop/xxxx/Uni/Kirigami_project/Keypoint_detection_notebooks/.Keypoint_detection_notebooks/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# importing relevant packages \n",
    "\n",
    "import torch \n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import random\n",
    "import albumentations as A\n",
    "import glob\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac46ccac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.34 11.33 21.02 22.05 23.   22.84 22.91 21.67 20.34 17.07  4.7  13.39\n",
      "  5.89 14.12 19.84 21.18 22.72 22.19 22.35 19.72 20.22 23.16 22.85 20.89\n",
      " 17.8  20.35 22.84 23.58 22.62 21.35 21.75 19.13 22.19 21.54 23.15 21.62\n",
      " 20.05 19.24 21.97 18.47]\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "dataset_names = ['Grasp_dataset_cylinder_20', 'Grasp_dataset_cylinder_40', 'Grasp_dataset_cylinder_60', 'Grasp_dataset_square_20', 'Grasp_dataset_square_40', 'Grasp_dataset_square_60']\n",
    "header = ['pressure_reading_1', 'pressure_reading_2', 'p1_x', 'p1_y', 'p2_x', 'p2_y', 'p3_x', 'p3_y', 'p4_x', 'p4_y', 'p5_x', 'p5_y', 'p6_x', 'p6_y', 'p7_x', 'p7_y', 'p8_x', 'p8_y', 'p9_x', 'p9_y', 'p10_x', 'p10_y', 'p11_x', 'p11_y', 'p12_x', 'p12_y', 'p13_x', 'p13_y', 'p14_x', 'p14_y', 'p15_x', 'p15_y', 'p16_x', 'p16_y']\n",
    "\n",
    "current_dir = os. getcwd()\n",
    "\n",
    "pressure_reading_1_arr = []\n",
    "pressure_reading_2_arr = []\n",
    "p1_x_arr = []\n",
    "p1_y_arr = []\n",
    "p2_x_arr = []\n",
    "p2_y_arr = []\n",
    "p3_x_arr = []\n",
    "p3_y_arr = []\n",
    "p4_x_arr = []\n",
    "p4_y_arr = []\n",
    "p5_x_arr = []\n",
    "p5_y_arr = []\n",
    "p6_x_arr = []\n",
    "p6_y_arr = []\n",
    "p7_x_arr = []\n",
    "p7_y_arr = []\n",
    "p8_x_arr = []\n",
    "p8_y_arr = []\n",
    "p9_x_arr = []\n",
    "p9_y_arr = []\n",
    "p10_x_arr = []\n",
    "p10_y_arr = []\n",
    "p11_x_arr = []\n",
    "p11_y_arr = []\n",
    "p12_x_arr = []\n",
    "p12_y_arr = []\n",
    "p13_x_arr = []\n",
    "p13_y_arr = []\n",
    "p14_x_arr = []\n",
    "p14_y_arr = []\n",
    "p15_x_arr = []\n",
    "p15_y_arr = []\n",
    "p16_x_arr = []\n",
    "p16_y_arr = []\n",
    "label_arr = []\n",
    "\n",
    "for f in dataset_names:\n",
    "    \n",
    "    image_dir = os.path.join(current_dir, f)\n",
    "    data_dir = os.path.join(image_dir, f + '.csv')\n",
    "    df = pd.read_csv(data_dir)\n",
    "    pressure_reading_1_arr = np.concatenate((pressure_reading_1_arr, df['pressure_reading_1'].values))\n",
    "    pressure_reading_2_arr = np.concatenate((pressure_reading_2_arr, df['pressure_reading_2'].values))\n",
    "    \n",
    "    print(pressure_reading_1_arr)\n",
    "    print(pressure_reading_1_arr[4])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f636d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec9046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ec15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4f900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8fe16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os. getcwd()\n",
    "source_images_dir = os.path.join(current_dir, 'Kirigami_dataset_2_3_400')\n",
    "source_df = pd.read_csv(os.path.join(source_images_dir, 'Kirigami_dataset_2_3_400.csv'))\n",
    "\n",
    "source_df = source_df.drop(columns=[\"image_name\"], axis=1)\n",
    "X = source_df\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38db5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random dataset for demonstration purposes\n",
    "np.random.seed(1)\n",
    "# X = np.random.rand(100, 5)\n",
    "y1 = np.repeat('square_60', 100)\n",
    "y2 = np.repeat('square_20', 50)\n",
    "y3 = np.repeat('square_60', 100)\n",
    "y4 = np.repeat('square_20', 100)\n",
    "y5 = np.repeat('square_20', 80)\n",
    "y6 = np.repeat('square_60', 50)\n",
    "y = np.concatenate([y1, y2, y3, y4, y5, y6])\n",
    "# X = pd.concat([pd.Series(np.random.randint(5, size = 480)), pd.Series(np.random.randint(5, size = 480)), pd.Series(np.random.randint(5, size = 480))], axis=1, keys='label')\n",
    "y = pd.concat([pd.Series(y)], axis=1, keys='label')\n",
    "# print(X.head(3))\n",
    "# y = np.random.randint(2, size=480)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# print(X_train.head(3))\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat('square_60', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5b76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38aa319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271eba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2570f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844db889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a5f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os. getcwd()\n",
    "images_dir = os.path.join(current_dir, 'Kirigami_dataset_2_3_2000')\n",
    "image_data = pd.read_csv(os.path.join(images_dir, 'Kirigami_dataset_2_3_2000.csv'))\n",
    "source_images_dir = os.path.join(current_dir, 'Kirigami_dataset_2_3_400')\n",
    "source_df = pd.read_csv(os.path.join(source_images_dir, 'Kirigami_dataset_2_3_400.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755745b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        label = self.df.iloc[idx, 1:]\n",
    "        image = self.transform_image(image)\n",
    "        return image.to(device), torch.tensor(label).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def transform_image(self, img):\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_mean = img_tensor.mean(dim = (1,2))\n",
    "        img_std = img_tensor.std(dim = (1,2))\n",
    "        \n",
    "        img_normalised = transforms.Normalize(img_mean, img_std)(img_tensor)\n",
    "        return img_normalised\n",
    "    \n",
    "    def load_img(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        img = cv2.imread(img_path)\n",
    "        return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data():\n",
    "    \n",
    "    files = glob.glob(os.path.join(images_dir, '*'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    \n",
    "    \n",
    "    header = ['image_name', 'p1_x', 'p1_y', 'p2_x', 'p2_y', 'p3_x', 'p3_y', 'p4_x', 'p4_y', 'p5_x', 'p5_y', 'p6_x', 'p6_y',\n",
    "              'p7_x', 'p7_y', 'p8_x', 'p8_y', 'p9_x', 'p9_y', 'p10_x', 'p10_y', 'p11_x', 'p11_y', 'p12_x', 'p12_y', 'p13_x',\n",
    "              'p13_y', 'p14_x', 'p14_y', 'p15_x', 'p15_y', 'p16_x', 'p16_y']\n",
    "    new_images_df = pd.DataFrame(columns=header)\n",
    "    \n",
    "    for j in range(0, 3):\n",
    "    \n",
    "        i = 0\n",
    "        repeat = 0\n",
    "    \n",
    "        while i < source_df.shape[0]:\n",
    "    \n",
    "            transform = A.Compose([\n",
    "                A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1),\n",
    "                A.RandomBrightnessContrast(brightness_limit=[-0.01, 0.01], contrast_limit=0.3, p=1),\n",
    "                A.Affine(rotate=random.uniform(-2, 2), p=1),\n",
    "                A.Affine(translate_percent={'x': random.uniform(-0.05, 0.05), 'y': random.uniform(-0.05, 0.05)}, p=1),\n",
    "                A.Affine(shear={'x': random.uniform(-2, 2), 'y': random.uniform(-1, 1)}, p=1),\n",
    "                A.Affine(scale=(0.96, 1.02), p=1)\n",
    "            ], keypoint_params=A.KeypointParams(format='xy'))\n",
    "    \n",
    "            image_to_be_transformed = cv2.imread(os.path.join(source_images_dir, source_df.iloc[i]['image_name']))\n",
    "    \n",
    "            x_points = source_df.iloc[i][1:][::2] * image_to_be_transformed.shape[1]\n",
    "            y_points = source_df.iloc[i][2:][::2] * image_to_be_transformed.shape[0]\n",
    "            albumentations_keypoints = list(zip(x_points, y_points))\n",
    "            #         vis_keypoints(image_to_be_transformed, albumentations_keypoints, color=KEYPOINT_COLOR, diameter=15)\n",
    "    \n",
    "            transformed = transform(image=image_to_be_transformed, keypoints=albumentations_keypoints)\n",
    "            transformed_image = transformed['image']\n",
    "            transformed_keypoints = transformed['keypoints']\n",
    "    \n",
    "            #         vis_keypoints(transformed_image, transformed_keypoints, color=KEYPOINT_COLOR, diameter=15)\n",
    "    \n",
    "            #         break\n",
    "    \n",
    "            if repeat >= 20:\n",
    "                repeat = 0\n",
    "                i = i + 1\n",
    "                continue\n",
    "    \n",
    "            if len(transformed_keypoints) != 16:\n",
    "                print(source_df.iloc[i]['image_name'])\n",
    "                repeat = repeat + 1\n",
    "    \n",
    "                continue\n",
    "    \n",
    "            new_image_name = str(j) + source_df.iloc[i]['image_name']\n",
    "            new_images_df.loc[len(new_images_df)] = new_image_name\n",
    "    \n",
    "            flatten_coordinates = np.array(transformed_keypoints).flatten()\n",
    "            flatten_coordinates[::2] = [x / transformed_image.shape[1] for x in flatten_coordinates[::2]]\n",
    "            flatten_coordinates[1::2] = [x / transformed_image.shape[0] for x in flatten_coordinates[1::2]]\n",
    "    \n",
    "            new_images_df.iloc[len(new_images_df) - 1, 1:33] = flatten_coordinates\n",
    "    \n",
    "            transformed_image[..., [0, 2]] = transformed_image[..., [2, 0]]\n",
    "            img = Image.fromarray(np.uint8(transformed_image))\n",
    "            img.save(os.path.join(images_dir, new_image_name))\n",
    "    \n",
    "            repeat = 0\n",
    "            i = i + 1\n",
    "    \n",
    "    new_images_df.to_csv(os.path.join(images_dir, 'Kirigami_dataset_2_3_2000.csv'), index=False)\n",
    "    \n",
    "    print(\"Finished\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95911c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_training_set():\n",
    "    global train_df, test_df, train_dataset, test_dataset, train_dataloader, test_dataloader, image_data\n",
    "    image_data = pd.read_csv(os.path.join(images_dir, 'Kirigami_dataset_2_3_2000.csv'))\n",
    "    train_df, test_df = train_test_split(image_data, test_size=0.04)\n",
    "    train_dataset = ImageDataset(train_df, images_dir)\n",
    "    test_dataset = ImageDataset(test_df, images_dir)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63269c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_training_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29521e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, points in train_dataloader:\n",
    "  print(img.shape)\n",
    "  print(points.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.Sequential(\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3200, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(300, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(pretrained=False)\n",
    "model = VGG_model()\n",
    "\n",
    "# from torchvision.models import resnet50\n",
    "# model = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.activation import Sigmoid\n",
    "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
    "pool_layer = nn.Sequential(\n",
    "     nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "     nn.LeakyReLU(0.1,inplace=True),\n",
    "     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "     nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "     nn.LeakyReLU(0.1,inplace=True),\n",
    "     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "     nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    ")\n",
    "\n",
    "model.avgpool = pool_layer\n",
    "\n",
    "final_predictor = nn.Sequential(\n",
    "    nn.Linear(3200, 300),\n",
    "    nn.LeakyReLU(0.1,inplace=True),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(300, 32),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.classifier = final_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./Model_2_2000_VGG_Leaky_60_epoch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.modules.activation import Sigmoid\n",
    "# from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
    "# # pool_layer = nn.Sequential(\n",
    "# #      nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "# #      nn.ReLU(inplace=True),\n",
    "# #      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "# #      nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "# #      nn.ReLU(inplace=True),\n",
    "# #      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "# #      nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    "# # )\n",
    "\n",
    "# # model.avgpool = pool_layer\n",
    "\n",
    "# final_predictor = nn.Sequential(\n",
    "#     nn.Linear(2048, 300),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(300, 32),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# model.fc = final_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56db79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, torch.rand(1,3,224,224))\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580657a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the optimizer and loss_function \n",
    "\n",
    "def get_essentials():\n",
    "  loss_fun = nn.L1Loss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "  return loss_fun, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining batch_train and accuracy functions\n",
    "\n",
    "\n",
    "def train_batch(data, model, loss_fun, optimizer):\n",
    "  model.train()\n",
    "  img, true_points = data\n",
    "  pred_points = model(img)\n",
    "  loss_val = loss_fun(pred_points, true_points)\n",
    "  loss_val.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  return loss_val.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(data, model, loss_fun, optimizer):\n",
    "  model.eval()\n",
    "  img, true_points = data\n",
    "  pred_points = model(img)\n",
    "  loss_val = loss_fun(pred_points, true_points)\n",
    "  return loss_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 401\n",
    "loss_fun, optimizer = get_essentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34af8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loops \n",
    "\n",
    "\n",
    "train_epoch, val_epoch = [], []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  train_batch_losses, val_batch_losses = [], []\n",
    "  for data in train_dataloader:\n",
    "    train_batch_loss = train_batch(data, model, loss_fun, optimizer)\n",
    "    train_batch_losses.append(train_batch_loss)\n",
    "  for data in test_dataloader:\n",
    "    val_batch_loss = val_batch(data, model, loss_fun, optimizer)\n",
    "    val_batch_losses.append(val_batch_loss)\n",
    "  train_epoch.append(np.mean(train_batch_losses))\n",
    "  val_epoch.append(np.mean(val_batch_losses))\n",
    "  \n",
    "  with open(\"Model_2_3_2000_output.txt\", \"a\") as f:\n",
    "    f.write(str([train_batch_loss, val_batch_loss]))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "  print(train_batch_loss, val_batch_loss)\n",
    "\n",
    "  if (epoch) % 20 == 0:\n",
    "    torch.save(model.state_dict(), './Model_2_3_2000_' + str(epoch) + '_epoch')\n",
    "    update_data()\n",
    "    update_training_set()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './Model_2800_VGG_Leaky_80_epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42bd5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_epoch, label=\"train_loss\")\n",
    "plt.plot(range(epochs), val_epoch, label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Facial Keypoints model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94e046",
   "metadata": {},
   "source": [
    "# Displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img_tensor = transforms.ToTensor()(img)\n",
    "    img_mean = img_tensor.mean(dim = (1,2))\n",
    "    img_std = img_tensor.std(dim = (1,2))\n",
    "        \n",
    "    img_normalised = transforms.Normalize(img_mean, img_std)(img_tensor)\n",
    "    return img_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(os.getcwd(), 'Validation_2/image11.jpg')\n",
    "original_img = cv2.imread(img_path)\n",
    "test_img = transform_image(original_img)\n",
    "test_img = test_img.to(device)\n",
    "\n",
    "Facial_key_points = model(test_img[None]).flatten().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c96698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "original_img = original_img[:,:,::-1]                                                              \n",
    "plt.imshow(original_img)\n",
    "plt.subplot(122)\n",
    "plt.title(\" Image with Keypoints \")\n",
    "plt.imshow(original_img)\n",
    "\n",
    "x_points = Facial_key_points.numpy()[0::2]\n",
    "y_points = Facial_key_points.numpy()[1::2]\n",
    "plt.scatter(x_points*original_img.shape[1], y_points*original_img.shape[0], s = 2, c=(1,0,0))                       # scaling the keypoints with image dimension\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Keypoint_detection_notebooks",
   "language": "python",
   "name": ".keypoint_detection_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
