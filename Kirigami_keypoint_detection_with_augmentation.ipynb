{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd2020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate\n",
    "# brightness\n",
    "# pixel drop\n",
    "# translate\n",
    "# rgb shift\n",
    "# contrast\n",
    "# blur\n",
    "# shear\n",
    "# lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cbb8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages \n",
    "\n",
    "import torch \n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db81ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os. getcwd()\n",
    "images_dir = os.path.join(current_dir, 'Kirigami_dataset_2_1000')\n",
    "image_data = pd.read_csv(os.path.join(images_dir, 'Kirigami_dataset_2_1000.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755745b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_name      p1_x      p1_y      p2_x      p2_y      p3_x      p3_y  \\\n",
      "0  0image0000.jpg  0.115423  0.277946  0.512124  0.081549  0.900808  0.260981   \n",
      "1  0image0001.jpg  0.103103  0.303101  0.482414  0.106966  0.862417  0.290306   \n",
      "2  0image0002.jpg  0.092324  0.301343  0.489235  0.156462  0.875456  0.289927   \n",
      "\n",
      "       p4_x      p4_y      p5_x  ...     p12_x     p12_y     p13_x     p13_y  \\\n",
      "0  0.907124  0.579452  0.517751  ...  0.578983  0.708468  0.594503  0.567082   \n",
      "1  0.866217  0.611283  0.502872  ...  0.561157  0.706985  0.568230  0.587686   \n",
      "2  0.874770  0.626714  0.490583  ...  0.550483  0.746341  0.563499  0.606076   \n",
      "\n",
      "      p14_x     p14_y     p15_x     p15_y     p16_x     p16_y  \n",
      "0  0.600941  0.430478  0.588429  0.295282  0.567786  0.161356  \n",
      "1  0.571716  0.452370  0.559542  0.319492  0.539509  0.183248  \n",
      "2  0.571159  0.464553  0.560680  0.328748  0.545606  0.196456  \n",
      "\n",
      "[3 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(image_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1b4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da77edd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0image0001.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.iloc[image_idx]['image_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde35fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_arr = cv2.imread(os.path.join(images_dir, image_data.iloc[image_idx]['image_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16cdbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1944, 2592, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f31cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img_arr)\n",
    "x_points = image_data.iloc[image_idx][1:][::2]\n",
    "y_points = image_data.iloc[image_idx][2:][::2]\n",
    "x_width = img_arr.shape[1]\n",
    "y_height = img_arr.shape[0]\n",
    "\n",
    "plt.scatter(x_points*x_width, y_points*y_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        label = self.df.iloc[idx, 1:]\n",
    "        image = self.transform_image(image)\n",
    "        return image.to(device), torch.tensor(label).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def transform_image(self, img):\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_mean = img_tensor.mean(dim = (1,2))\n",
    "        img_std = img_tensor.std(dim = (1,2))\n",
    "        \n",
    "        img_normalised = transforms.Normalize(img_mean, img_std)(img_tensor)\n",
    "        return img_normalised\n",
    "    \n",
    "    def load_img(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        img = cv2.imread(img_path)\n",
    "        return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os. getcwd()\n",
    "images_dir = os.path.join(current_dir, 'Kirigami_dataset_2_1000')\n",
    "print(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(image_data, test_size=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b12f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, images_dir)\n",
    "test_dataset = ImageDataset(test_df, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3006c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = test_dataset.load_img(0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor, labels = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2dba9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ce37a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_np = img_tensor.cpu().numpy()\n",
    "img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2ac7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(img_np.shape)\n",
    "img_np_reverted = np.transpose(img_np, (1,2,0))\n",
    "print(img_np_reverted.shape)\n",
    "plt.imshow(img_np_reverted, clim=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02387cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29521e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, points in train_dataloader:\n",
    "  print(img.shape)\n",
    "  print(points.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_model, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1,inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.Sequential(\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3200, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(300, 6),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(pretrained=False)\n",
    "model = VGG_model()\n",
    "\n",
    "# from torchvision.models import resnet50\n",
    "# model = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making the weights non trainable\n",
    "\n",
    "# for param in model.parameters():\n",
    "#   param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.activation import Sigmoid\n",
    "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
    "pool_layer = nn.Sequential(\n",
    "     nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "     nn.LeakyReLU(0.1,inplace=True),\n",
    "     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "     nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "     nn.LeakyReLU(0.1,inplace=True),\n",
    "     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "     nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    ")\n",
    "\n",
    "model.avgpool = pool_layer\n",
    "\n",
    "final_predictor = nn.Sequential(\n",
    "    nn.Linear(3200, 300),\n",
    "    nn.LeakyReLU(0.1,inplace=True),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(300, 32),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.classifier = final_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./Model_2_2000_VGG_Leaky_60_epoch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.modules.activation import Sigmoid\n",
    "# from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
    "# # pool_layer = nn.Sequential(\n",
    "# #      nn.Conv2d(512,512, kernel_size=3, padding='same'),\n",
    "# #      nn.ReLU(inplace=True),\n",
    "# #      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "# #      nn.Conv2d(512,50, kernel_size=3, padding='same'),\n",
    "# #      nn.ReLU(inplace=True),\n",
    "# #      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "# #      nn.AdaptiveAvgPool2d(output_size=(8,8))\n",
    "# # )\n",
    "\n",
    "# # model.avgpool = pool_layer\n",
    "\n",
    "# final_predictor = nn.Sequential(\n",
    "#     nn.Linear(2048, 300),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(300, 32),\n",
    "#     nn.Sigmoid()\n",
    "# )\n",
    "\n",
    "# model.fc = final_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56db79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, torch.rand(1,3,224,224))\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580657a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the optimizer and loss_function \n",
    "\n",
    "def get_essentials():\n",
    "  loss_fun = nn.L1Loss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "  return loss_fun, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50da60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining batch_train and accuracy functions\n",
    "\n",
    "\n",
    "def train_batch(data, model, loss_fun, optimizer):\n",
    "  model.train()\n",
    "  img, true_points = data\n",
    "  pred_points = model(img)\n",
    "  loss_val = loss_fun(pred_points, true_points)\n",
    "  loss_val.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  return loss_val.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_batch(data, model, loss_fun, optimizer):\n",
    "  model.eval()\n",
    "  img, true_points = data\n",
    "  pred_points = model(img)\n",
    "  loss_val = loss_fun(pred_points, true_points)\n",
    "  return loss_val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 61\n",
    "loss_fun, optimizer = get_essentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34af8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loops \n",
    "\n",
    "\n",
    "train_epoch, val_epoch = [], []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  train_batch_losses, val_batch_losses = [], []\n",
    "  for data in train_dataloader:\n",
    "    train_batch_loss = train_batch(data, model, loss_fun, optimizer)\n",
    "    train_batch_losses.append(train_batch_loss)\n",
    "  for data in test_dataloader:\n",
    "    val_batch_loss = val_batch(data, model, loss_fun, optimizer)\n",
    "    val_batch_losses.append(val_batch_loss)\n",
    "  train_epoch.append(np.mean(train_batch_losses))\n",
    "  val_epoch.append(np.mean(val_batch_losses))\n",
    "  \n",
    "  with open(\"Model_2_1000_output.txt\", \"a\") as f:\n",
    "    f.write(str([train_batch_loss, val_batch_loss]))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "  print(train_batch_loss, val_batch_loss)\n",
    "\n",
    "  if (100+epoch) % 20 == 0:\n",
    "    torch.save(model.state_dict(), './Model_2_1000_VGG_Leaky_' + str(100+epoch) + '_epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Model_2800_VGG_Leaky_80_epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42bd5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_epoch, label=\"train_loss\")\n",
    "plt.plot(range(epochs), val_epoch, label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Facial Keypoints model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94e046",
   "metadata": {},
   "source": [
    "# Displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img_tensor = transforms.ToTensor()(img)\n",
    "    img_mean = img_tensor.mean(dim = (1,2))\n",
    "    img_std = img_tensor.std(dim = (1,2))\n",
    "        \n",
    "    img_normalised = transforms.Normalize(img_mean, img_std)(img_tensor)\n",
    "    return img_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(os.getcwd(), 'Validation_2/image11.jpg')\n",
    "original_img = cv2.imread(img_path)\n",
    "test_img = transform_image(original_img)\n",
    "test_img = test_img.to(device)\n",
    "\n",
    "Facial_key_points = model(test_img[None]).flatten().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c96698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "original_img = original_img[:,:,::-1]                                                              \n",
    "plt.imshow(original_img)\n",
    "plt.subplot(122)\n",
    "plt.title(\" Image with Keypoints \")\n",
    "plt.imshow(original_img)\n",
    "\n",
    "x_points = Facial_key_points.numpy()[0::2]\n",
    "y_points = Facial_key_points.numpy()[1::2]\n",
    "plt.scatter(x_points*original_img.shape[1], y_points*original_img.shape[0], s = 2, c=(1,0,0))                       # scaling the keypoints with image dimension\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Facial_key_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = Facial_key_points.numpy()[0::2]\n",
    "y_points = Facial_key_points.numpy()[1::2]\n",
    "print(x_points)\n",
    "print(y_points)\n",
    "\n",
    "# width = img_arr.shape[0]\n",
    "# height = img_arr.shape[1]\n",
    "\n",
    "# plt.scatter(x_points*x_width, y_points*y_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    folder = 'asdsadsd'\n",
    "    print(folder + str(i).zfill(2)+'.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Keypoint_detection_notebooks",
   "language": "python",
   "name": ".keypoint_detection_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
